{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load RL Policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Policy\n",
    "\n",
    "### Policy Options\n",
    "\n",
    "- [Documentation](https://docs.ray.io/en/latest/rllib/rllib-saving-and-loading-algos-and-policies.html)\n",
    "- `policy_dir` is the path to the last version of our policy and its weights\n",
    "- `checkpoint_dir` is the path to the last checkpoint from our training process. \n",
    "- We won't be using `checkpoint_dir`, but it's useful for restoring to the previous state and configuration, allowing us to continue training if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#ef4444\">\n",
    "&#x2B55; When loading in these checkpoints it's best to use the same version of python used in the training environment.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"sumo_3d_demo/ray_results/Sheridan/PPO/PPO_Sheridan_0c5d9_00000_0_2024-11-26_15-42-31/checkpoint_000018\"\n",
    "\n",
    "policy_dir = \"sumo_3d_demo/ray_results/Sheridan/PPO/PPO_Sheridan_0c5d9_00000_0_2024-11-26_15-42-31/checkpoint_000018/policies/default_policy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to load any policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPOTorchPolicy\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "policy = Policy.from_checkpoint(policy_dir)\n",
    "\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to load any Algorithm checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "algorithm = Algorithm.from_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PPO policy for SUMO\n",
    "\n",
    "\n",
    "### Explanation of the Code\n",
    "\n",
    "1. **Import Required Libraries**:  \n",
    "   - `sumo_rl`: Used to interact with the SUMO simulation environment.  \n",
    "   - `ray.rllib.policy`: For loading and using a pre-trained RL policy.  \n",
    "   - `numpy`: Handles numerical operations like array transformations.  \n",
    "\n",
    "2. **Load Pre-trained Policy**:  \n",
    "   - The policy is loaded from our last checkpoint using `Policy.from_checkpoint(policy_dir)`.\n",
    "\n",
    "3. **Initialize SUMO-RL Environment**:  \n",
    "   - The SUMO simulation requires a network file (`osm.net.xml`) and a route file (`osm.rou.xml`).  \n",
    "   - GUI is enabled, and the simulation runs for up to 80,000 seconds.\n",
    "\n",
    "4. **Environment Reset**:  \n",
    "   - The environment is reset using `env.reset()`, providing initial observations (`observations`) for all agents.\n",
    "\n",
    "5. **Action Loop**:  \n",
    "   - While agents are active in the environment:  \n",
    "     - **Action Computation**: For each agent:  \n",
    "       - Observations are converted to a NumPy array (`obs_array`) for compatibility.  \n",
    "       - The pre-trained policy computes actions based on observations using `policy.compute_single_action()`.  \n",
    "     - **Environment Step**:  \n",
    "       - The computed actions are passed to the environment using `env.step(actions)`, which returns:  \n",
    "         - `observations`: New state information for agents.  \n",
    "         - `rewards`: Immediate rewards for actions.  \n",
    "         - `terminations` and `truncations`: Flags indicating if agents have completed or are truncated.  \n",
    "         - `infos`: Additional environment data.  \n",
    "\n",
    "6. **Reward Logging**:  \n",
    "   - Rewards for each agent are printed for monitoring performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumo_rl\n",
    "import numpy as np\n",
    "from ray.rllib.policy.policy import Policy\n",
    "\n",
    "policy = Policy.from_checkpoint(policy_dir)\n",
    "\n",
    "# Initialize SUMO-RL environment\n",
    "env = sumo_rl.parallel_env(\n",
    "    net_file='sumo_3d_demo/osm.net.xml',\n",
    "    route_file='sumo_3d_demo/osm.rou.xml',\n",
    "    use_gui=True,\n",
    "    num_seconds=80000\n",
    ")\n",
    "\n",
    "# Reset the environment\n",
    "observations, infos = env.reset()\n",
    "\n",
    "while env.agents:\n",
    "    # Compute actions using the loaded policy\n",
    "    actions = {}\n",
    "    for agent, obs in observations.items():\n",
    "        # Convert observation to appropriate format\n",
    "        obs_array = np.array(obs).astype(np.float32)\n",
    "        \n",
    "        # Compute the action using the policy\n",
    "        # Extract only the action\n",
    "        action, _, _ = policy.compute_single_action(obs_array) \n",
    "\n",
    "        actions[agent] = action\n",
    "    \n",
    "    # Step the environment\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    \n",
    "    # Optional: Log rewards or other metrics\n",
    "    for agent, reward in rewards.items():\n",
    "        print(f\"Agent: {agent}, Reward: {reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
