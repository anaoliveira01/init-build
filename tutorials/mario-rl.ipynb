{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcb2f90-9051-4238-bb43-8236353a974d",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Mario RL tutorial</h1>\n",
    "\n",
    "---\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "    <time datetime=\"2024-09-22\">September 22, 2024</time> | <span>Topics: Mario, Stablebaseline, and simulated enviroments</span>\n",
    "    <br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592009ff-6975-47e0-a4e0-619c2d76b737",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "\n",
    "In this tutorial, we'll teach an AI to play **Super Mario Bros** using reinforcement learning. By interacting with the game, the AI will learn which actions lead to rewards and which do not, gradually improving its performance. We'll be using tools like **Stable-baselines3**, **OpenAI Gym**, and **Pytorch** to build and train our model in a simulated environment.\n",
    "\n",
    "Reinforcement learning is a powerful technique for training AI agents in environments where they learn from trial and error. This project will help you understand key concepts of reinforcement learning and how to apply them to real-world problems, like playing a video game autonomously.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "- Set up the Mario environment using OpenAI Gym.\n",
    "- Train an AI model to play Mario using reinforcement learning.\n",
    "- Evaluate the performance of the model as it interacts with the game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd77125-24b4-449b-bded-e5faeda37553",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\">Don't feel like reading? still not clear? you can listen what we'll be working on</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9a65de-4411-48d6-8d3d-dcf8a030edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT THIS CELL TO LISTEN TO THE AUDIO FILE\n",
    "\n",
    "# from IPython.display import Audio\n",
    "# audio_file = 'public/mario.wav'\n",
    "# Audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa515495-102a-4cab-b671-4bf2c4ec7e40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837b65e3",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "- Python == 3.8 \n",
    "- pytorch\n",
    "- gym == 0.21.0\n",
    "- nes_py ==  8.2.1\n",
    "- stable-baseline3[extra] == 1.6.0\n",
    "- Jupyter notebook\n",
    "- [PyTorch — click link and scroll down to install](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbc078-8d08-48a8-a541-203a9c1931d2",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7050d7-1d81-482f-a2c9-f72934c31f28",
   "metadata": {},
   "source": [
    "Before we start, we need to install a few packages. Here's a brief overview of each:\n",
    "- **`nes-py`**: Provides an NES emulator environment used to run the Super Mario Bros. game in Python.\n",
    "- **`gym-super-mario-bros`**: Integrates the Super Mario Bros. game into OpenAI's Gym framework for use in reinforcement learning tasks.\n",
    "- **`gym`**: Provides environments for developing and comparing reinforcement learning algorithms.\n",
    "- **`stable-baselines3[extra]`**: A popular library that implements advanced reinforcement learning algorithms, including extra features like visualization and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd9431-35f2-4af0-8006-591499723964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "packages = [\n",
    "    \"setuptools==65.5.0\", \"wheel<0.40.0\",\n",
    "    \"gym==0.21.0\",\n",
    "    \"stable-baselines3[extra]==1.6.0\",\n",
    "    \"nes-py\",\n",
    "    \"gym-super-mario-bros==7.3.0\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    subprocess.check_call([\"pip\", \"install\", package])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b444e1-384b-4118-ae88-705b284a94fe",
   "metadata": {},
   "source": [
    "<span style=\"color:#ef4444\">\n",
    "&#x2B55; If you get this error: 'no matches found'\n",
    "    for the stable-baseline 3 package, you may have to run: pip install 'stable-baselines3[extra]'\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de28cfc-6426-4b72-a3fd-c20d8b167471",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a5af76-3236-486f-be2d-de16f5e1283b",
   "metadata": {},
   "source": [
    "## Setting up Mario Environment\n",
    "\n",
    "### Objective\n",
    "- Set up the environment where our AI can interact with and learn to play Super Mario Bros.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "To teach our AI how to play Super Mario Bros, we first need to set up the environment in which it will learn and play. We'll use specific libraries to simulate the game and restrict the AI's actions for more efficient learning.\n",
    "\n",
    "- **Why use `gym_super_mario_bros`?**  \n",
    "  We use `gym_super_mario_bros` to create a simulation of the game. This provides an environment where the AI can interact with the game, observe the outcomes, and learn from its actions.\n",
    "\n",
    "- **Why use `JoypadSpace`?**  \n",
    "  Super Mario Bros. has many possible actions (like jumping, running, etc.), but we simplify this by using `JoypadSpace`. This reduces the action space to a smaller set of key actions (defined in `SIMPLE_MOVEMENT`), making it easier for our AI to learn how to play.\n",
    "\n",
    "- **How does the environment work?**  \n",
    "  - **`gym_super_mario_bros.make('SuperMarioBros-v0')`**: This function sets up the game environment.\n",
    "  - **`JoypadSpace`**: Wraps the environment, limiting the AI's actions to a simpler, predefined set of movements, which helps the AI focus on learning key behaviors.\n",
    "\n",
    "### Outcome\n",
    "After setting up the environment, our AI is now ready to interact with the game. It can start taking actions within the simplified movement space, and you'll be able to see the game being played in a simulated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6dfa88-718c-48c4-8975-8cac92d77913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb24ce0-7e55-443e-81a7-163ef0cc0981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the possible options for our AI agent?\n",
    "SIMPLE_MOVEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bb7528-bb5c-4151-a738-d184e294549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the game environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "# wrap the environment in JoypadSpace, reducing the possible actions by our AI agent\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750bb2cb-aeb9-48f7-87dd-861a0a05f246",
   "metadata": {},
   "source": [
    "#### Let's test our game!\n",
    "\n",
    "**Code Breakdown**:\n",
    "\n",
    "- **`for step in range(1000)`**: Runs the game for 1000 steps.\n",
    "- **`env.reset()`**: Resets the game when it ends.\n",
    "- **`env.step(env.action_space.sample())`**: Takes a random action from the AI's action space.\n",
    "- **`state, reward, done, info`**: After each action, the game returns:\n",
    "  - **`state`**: The current game state.\n",
    "  - **`reward`**: The reward for the action taken.\n",
    "  - **`done`**: Whether the game has ended.\n",
    "  - **`info`**: Extra diagnostic information.\n",
    "- **`env.render()`**: Renders the game so you can see it in action.\n",
    "- **`env.close()`**: Closes the game when the loop ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0c86d-82af-4e32-bc55-8992bdf2d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test if our game is working!\n",
    "done = True\n",
    "for step in range(1000):\n",
    "\tif done:\n",
    "\t\tenv.reset()\n",
    "\tstate, reward, done , info = env.step(env.action_space.sample())\n",
    "\tenv.render()\t\t\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc58a1e-893d-4990-b03f-aadfa6d7b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to close any leftover windows from the last cell\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119b4df-4a66-48b0-86c0-7a67d1af8d11",
   "metadata": {},
   "source": [
    "<span style=\"color:#ef4444\">\n",
    "&#x2B55; If you see an overflow warning but your game is running ok, you can ignore it.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64387c-fa2f-43d2-99fd-dd30c7cc1b38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e11474a",
   "metadata": {},
   "source": [
    "## Frame Stacking and Grayscale Processing\n",
    "\n",
    "### Objective\n",
    "Prepare the game environment by simplifying the visual inputs (grayscale) and stacking frames to give the AI temporal information. This will help the AI learn better by providing sequences of images instead of a single frame.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "- **`GrayScaleObservation`**: Converts the game images to grayscale, which simplifies the visual input while retaining important features for the AI.\n",
    "- **`VecFrameStack`**: Stacks multiple consecutive frames together, allowing the AI to capture motion and temporal information. This helps in understanding movement, such as jumps and enemy approaches.\n",
    "- **`DummyVecEnv`**: Wraps the environment to make it compatible with vectorized operations, required for `stable-baselines3`.\n",
    "- **`plt.imshow`**: Visualizes the game state to ensure everything is working properly.\n",
    "- **`plt.subplot`**: Displays the 4 stacked frames, allowing you to visualize the temporal information given to the AI.\n",
    "\n",
    "### Outcome\n",
    "The environment now processes images in grayscale and stacks four frames together, which helps the AI understand motion and time-based actions. You'll also be able to visualize the stacked frames to verify that they contain temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccd9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e29f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "# Grayscale the image\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "# Wrap inside a DummyVecEnv\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# Stack the frames\n",
    "env = VecFrameStack(env, 4, channels_order='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360baac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start game in the background\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0bd1ca-ff99-4b57-b751-e578db45e61d",
   "metadata": {},
   "source": [
    "#### Let's analyze the environment state\n",
    "\n",
    "**Code Breakdown**:\n",
    "\n",
    "- **`state.shape`**: Returns the shape of the `state` variable, which is:\n",
    "  - **`(1, 240, 256, 4)`**: \n",
    "    - **`1`**: The batch dimension, which allows for multiple environments in parallel (in this case, only one environment).\n",
    "    - **`240, 256`**: The height and width of the game screen in pixels.\n",
    "    - **`4`**: The number of stacked frames, which helps the AI capture motion and temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b448ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ab32f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make mario jump and add to our temporal state variable (run this line 4 time)\n",
    "state, reward, done , info = env.step([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize one frame\n",
    "plt.imshow(state[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c3aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the AI agent see?\n",
    "plt.figure(figsize=(20, 16))\n",
    "for idx in range(state.shape[3]):\n",
    "\tplt.subplot(1, 4, idx+1)\n",
    "\tplt.imshow(state[0][:, :, idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d5707",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Objective\n",
    "Create a reinforcement learning model using Proximal Policy Optimization (PPO) to teach our AI how to play Super Mario Bros, and implement a callback system for saving the model during training.\n",
    "\n",
    "### Step-by-Step Explanation\n",
    "\n",
    "- **`PPO` (Proximal Policy Optimization)**: This is the reinforcement learning algorithm we are using. It optimizes the policy that controls the AI’s actions in the game by gradually improving its performance through interaction with the environment.\n",
    "- **`TrainAndLoggingCallback`**: This custom callback saves the model at regular intervals (defined by `check_freq`). It helps ensure that we can save the model during training and restore it later if needed.\n",
    "- **`model.learn()`**: This function trains the model over a specified number of timesteps (in this case, 100,000). The model interacts with the game, learning from rewards and adjusting its strategy.\n",
    "  \n",
    "**Key components**:\n",
    "- **`PPO('CnnPolicy', env)`**: This creates the PPO model, using a convolutional neural network (CNN) policy to process image data from the game.\n",
    "- **`callback=TrainAndLoggingCallback`**: This ensures that the model is saved every 10,000 steps during training.\n",
    "- **`n_steps`**: Defines how often the agent updates its learning, impacting performance and speed.\n",
    "\n",
    "### Outcome\n",
    "The PPO model will now train the AI by interacting with the game and learning from its experiences. Additionally, the model is saved every 10,000 steps, allowing us to resume training or deploy the saved model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b677937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os for file path management\n",
    "import os\n",
    "# Import PPO for using Proximal Policy Optimization\n",
    "from stable_baselines3 import PPO\n",
    "# Import Base Callback for saving models\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d713b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback for saving models\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\tdef __init__(self, check_freq, save_path, verbose=1):\n",
    "\t\tsuper(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "\t\tself.check_freq = check_freq\n",
    "\t\tself.save_path = save_path\n",
    "\n",
    "\tdef _init_callback(self):\n",
    "\t\tif self.save_path is not None:\n",
    "\t\t\tos.makedirs(self.save_path, exist_ok=True)\n",
    "\t\t\n",
    "\tdef _on_step(self):\n",
    "\t\tif self.n_calls % self.check_freq == 0:\n",
    "\t\t\tmodel_path = os.path.join(self.save_path, f'best_model_{self.n_calls}')\n",
    "\t\t\tself.model.save(model_path)\n",
    "\n",
    "\t\treturn True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bfa66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "LOG_DIR = './logs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c41d2-c469-465f-bdb1-1a17f4ebdd68",
   "metadata": {},
   "source": [
    "#### Creating the PPO Model\n",
    "\n",
    "**Code Breakdown**:\n",
    "\n",
    "- **`PPO('CnnPolicy', env)`**: Initializes a PPO model with a CNN policy, designed to process image data from `env`.\n",
    "- **`verbose=1`**: Enables progress output during training.\n",
    "- **`tensorboard_log=LOG_DIR`**: Sets up logging for TensorBoard, allowing you to track training metrics.\n",
    "- **`learning_rate=0.000001`**: Defines the learning rate, which controls how quickly the model updates its knowledge.\n",
    "- **`n_steps=512`**: Specifies the number of steps the agent takes before updating its learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dd478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72594d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.learn(total_timesteps=100000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bea133",
   "metadata": {},
   "source": [
    "## 4. Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.utils import constant_fn\n",
    "\n",
    "model = PPO.load('train/[MODEL NAME]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865aa57-e5f3-4b52-bb07-ee420e9faf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d666e6ec-d40c-48c5-9308-dfcbcbc7078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the game \n",
    "state = env.reset()\n",
    "# Loop through the game\n",
    "while True: \n",
    "    \n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
